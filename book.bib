@web_page{,
   title = {Automated Interpretation of Indices of Effect Size • effectsize},
   url = {https://easystats.github.io/effectsize/articles/interpret.html},
}
@web_page{,
   title = {Comprehensive Meta-Analysis Software (CMA)},
   url = {https://www.meta-analysis.com/},
}
@article{Carter2019,
   abstract = {Publication bias and questionable research practices in primary research can lead to badly overestimated effects in meta-analysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses—that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shinyapps.org/apps/metaExplorer/.},
   author = {Evan C. Carter and Felix D. Schönbrodt and Will M. Gervais and Joseph Hilgard},
   doi = {10.1177/2515245919847196/ASSET/IMAGES/LARGE/10.1177_2515245919847196-FIG1.JPEG},
   issn = {25152467},
   issue = {2},
   journal = {Advances in Methods and Practices in Psychological Science},
   keywords = {bias correction,meta-analysis,open data,open materials,p-hacking,publication bias,questionable research practices},
   month = {6},
   pages = {115-144},
   publisher = {SAGE Publications Inc.},
   title = {Correcting for Bias in Psychology: A Comparison of Meta-Analytic Methods},
   volume = {2},
   url = {https://journals.sagepub.com/doi/10.1177/2515245919847196},
   year = {2019},
}
@web_page{,
   title = {RevMan | Cochrane Training},
   url = {https://training.cochrane.org/online-learning/core-software/revman},
}
@article{Denson2009,
   abstract = {Models of stress and health suggest that emotions mediate the effects of stress on health; yet meta-analytic reviews have not confirmed these relationships. Categorizations of emotions along broad dimensions such as valence (e.g., positive and negative affect) may obscure important information about the effects of specific emotions on physiology. Within the context of the integrated specificity model, we present a novel theoretical framework that posits that specific emotional responses associated with specific types of environmental demands influence cortisol and immune outcomes in a manner that would have likely promoted the survival of our ancestors. We analyzed experiments from 66 journal articles that directly manipulated social stress or emotions and measured subsequent cortisol or immune responses. Judges rated experiments for the extent to which participants would experience theoretically relevant cognition and affect clustered around five categories: (a) cognitive appraisals, (b) basic emotions, (c) rumination and worry, (d) social threat, and (e) global mood states. As expected, global mood states were unassociated with the effect sizes, whereas exemplars from the other categories were generally associated with effect sizes in the expected manner. The present research suggests that coping strategies that alter appraisals and emotional responses may improve long-term health outcomes. This might be especially relevant for stressors that are acute or imminent, threaten one's social status, or require extended effort. © 2009 American Psychological Association.},
   author = {Thomas F. Denson and Marija Spanovic and Norman Miller},
   doi = {10.1037/A0016909},
   issn = {00332909},
   issue = {6},
   journal = {Psychological Bulletin},
   keywords = {appraisal,cortisol,emotion,immune system,stress},
   month = {11},
   pages = {823-853},
   pmid = {19883137},
   title = {Cognitive Appraisals and Emotions Predict Cortisol and Immune Responses: A Meta-Analysis of Acute Laboratory Social Stressors and Emotion Inductions},
   volume = {135},
   url = {/doiLanding?doi=10.1037%2Fa0016909},
   year = {2009},
}
@article{Field2010,
   abstract = {Meta-analysis is a statistical tool for estimating the mean and variance of underlying population effects from a collection of empirical studies addressing ostensibly the same research question. Meta-analysis has become an increasing popular and valuable tool in psychological research, and major review articles typically employ these methods. This article describes the process of conducting meta-analysis: selecting articles, developing inclusion criteria, calculating effect sizes, conducting the actual analysis (including information on how to do the analysis on popular computer packages such as IBM SPSS and R) and estimating the effects of publication bias. Guidance is also given on how to write up a meta-analysis. © 2010 The British Psychological Society.},
   author = {Andy P. Field and Raphael Gillett},
   doi = {10.1348/000711010X502733},
   issn = {2044-8317},
   issue = {3},
   journal = {British Journal of Mathematical and Statistical Psychology},
   month = {11},
   pages = {665-694},
   pmid = {20497626},
   publisher = {John Wiley & Sons, Ltd},
   title = {How to do a meta-analysis},
   volume = {63},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1348/000711010X502733 https://onlinelibrary.wiley.com/doi/abs/10.1348/000711010X502733 https://bpspsychub.onlinelibrary.wiley.com/doi/10.1348/000711010X502733},
   year = {2010},
}
@article{Harrer2021,
   abstract = {Doing Meta-Analysis with R: A Hands-On Guide serves as an accessible introduction on how meta-analyses can be conducted in R. Essential steps for meta-analysis are covered, including calculation and pooling of outcome measures, forest plots, heterogeneity diagnostics, subgroup analyses, meta-regression, methods to control for publication bias, risk of bias assessments and plotting tools. Advanced but highly relevant topics such as network meta-analysis, multi-three-level meta-analyses, Bayesian meta-analysis approaches and SEM meta-analysis are also covered. A companion R package, dmetar, is introduced at the beginning of the guide. It contains data sets and several helper functions for the meta and metafor package used in the guide. 
The programming and statistical background covered in the book are kept at a non-expert level, making the book widely accessible. 
Features• Contains two introductory chapters on how to set up an R environment and do basic imports/manipulations of meta-analysis data, including exercises• Describes statistical concepts clearly and concisely before applying them in R• Includes step-by-step guidance through the coding required to perform meta-analyses, and a companion R package for the book},
   author = {Mathias Harrer and Pim Cuijpers and Toshi A. Furukawa and David D. Ebert},
   doi = {10.1201/9781003107347},
   isbn = {9781003107347},
   journal = {Doing Meta-Analysis with R},
   month = {9},
   publisher = {Chapman and Hall/CRC},
   title = {Doing Meta-Analysis with R : A Hands-On Guide},
   url = {https://www.taylorfrancis.com/books/mono/10.1201/9781003107347/meta-analysis-mathias-harrer-pim-cuijpers-toshi-furukawa-david-ebert},
   year = {2021},
}
@article{Higgins2011,
   abstract = {Flaws in the design, conduct, analysis, and reporting of randomised trials can cause the effect of an intervention to be underestimated or overestimated. The Cochrane Collaboration’s tool for assessing risk of bias aims to make the process clearer and more accurate

Randomised trials, and systematic reviews of such trials, provide the most reliable evidence about the effects of healthcare interventions. Provided that there are enough participants, randomisation should ensure that participants in the intervention and comparison groups are similar with respect to both known and unknown prognostic factors. Differences in outcomes of interest between the different groups can then in principle be ascribed to the causal effect of the intervention.1

Causal inferences from randomised trials can, however, be undermined by flaws in design, conduct, analyses, and reporting, leading to underestimation or overestimation of the true intervention effect (bias).2 However, it is usually impossible to know the extent to which biases have affected the results of a particular trial.

Systematic reviews aim to collate and synthesise all studies that meet prespecified eligibility criteria3 using methods that attempt to minimise bias. To obtain reliable conclusions, review authors must carefully consider the potential limitations of the included studies. The notion of study “quality” is not well defined but relates to the extent to which its design, conduct, analysis, and presentation were appropriate to answer its research question. Many tools for assessing the quality of randomised trials are available, including scales (which score the trials) and checklists (which assess trials without producing a score).4 5 6 7 Until recently, Cochrane reviews used a variety of these tools, mainly checklists.8 In 2005 the Cochrane Collaboration’s methods groups embarked on a new strategy for assessing the quality of randomised trials. In this paper we describe the collaboration’s new risk of bias …},
   author = {Julian P.T. Higgins and Douglas G. Altman and Peter C. Gøtzsche and Peter Jüni and David Moher and Andrew D. Oxman and Jelena Savović and Kenneth F. Schulz and Laura Weeks and Jonathan A.C. Sterne},
   doi = {10.1136/BMJ.D5928},
   issn = {0959-8138},
   issue = {7829},
   journal = {BMJ},
   month = {10},
   pmid = {22008217},
   publisher = {British Medical Journal Publishing Group},
   title = {The Cochrane Collaboration’s tool for assessing risk of bias in randomised trials},
   volume = {343},
   url = {https://www.bmj.com/content/343/bmj.d5928 https://www.bmj.com/content/343/bmj.d5928.abstract},
   year = {2011},
}
@web_page{,
   title = {JASP - A Fresh Way to Do Statistics},
   url = {https://jasp-stats.org/},
}
@web_page{,
   title = {jamovi - open statistical software for the desktop and cloud},
   url = {https://www.jamovi.org/},
}
@article{Kassai2019,
   abstract = {In the present meta-analysis we examined the near- and far-transfer effects of training components of children's executive functions skills: working memory, inhibitory control, and cognitive flexibility. We found a significant near-transfer effect (g+ = 0.44, k = 43, p < .001) showing that the interventions in the primary studies were successful in training the targeted components. However, we found no convincing evidence of far-transfer (g+ = 0.11, k = 17, p = .11). That is, training a component did not have a significant effect on the untrained components. By showing the absence of benefits that generalize beyond the trained components, we question the practical relevance of training specific executive function skills in isolation. Furthermore, the present results might explain the absence of far-transfer effects of working memory training on academic skills (Melby-Lervag & Hulme, 2013; Sala & Gobet, 2017).},
   author = {Reka Kassai and Judit Futo and Zsolt Demetrovics and Zsofia K. Takacs},
   doi = {10.1037/BUL0000180},
   issn = {00332909},
   issue = {2},
   journal = {Psychological Bulletin},
   keywords = {Children,Executive functions,Intervention,Meta-analysis},
   month = {2},
   pages = {165-188},
   pmid = {30652908},
   publisher = {American Psychological Association Inc.},
   title = {A meta-analysis of the experimental evidence on the near- and far-transfer effects among children's executive function skills},
   volume = {145},
   url = {/doiLanding?doi=10.1037%2Fbul0000180},
   year = {2019},
}
@article{Krippendorff2011,
   author = {Klaus Krippendorff},
   journal = {Departmental Papers (ASC)},
   month = {1},
   title = {Computing Krippendorff's Alpha-Reliability},
   url = {https://repository.upenn.edu/asc_papers/43},
   year = {2011},
}
@article{Lakens2016,
   abstract = {Background: Meta-analyses play an important role in cumulative science by combining information across multiple studies and attempting to provide effect size estimates corrected for publication bias. Research on the reproducibility of meta-analyses reveals that errors are common, and the percentage of effect size calculations that cannot be reproduced is much higher than is desirable. Furthermore, the flexibility in inclusion criteria when performing a meta-analysis, combined with the many conflicting conclusions drawn by meta-analyses of the same set of studies performed by different researchers, has led some people to doubt whether meta-analyses can provide objective conclusions. Discussion: The present article highlights the need to improve the reproducibility of meta-analyses to facilitate the identification of errors, allow researchers to examine the impact of subjective choices such as inclusion criteria, and update the meta-analysis after several years. Reproducibility can be improved by applying standardized reporting guidelines and sharing all meta-analytic data underlying the meta-analysis, including quotes from articles to specify how effect sizes were calculated. Pre-registration of the research protocol (which can be peer-reviewed using novel 'registered report' formats) can be used to distinguish a-priori analysis plans from data-driven choices, and reduce the amount of criticism after the results are known. Summary: The recommendations put forward in this article aim to improve the reproducibility of meta-analyses. In addition, they have the benefit of "future-proofing" meta-analyses by allowing the shared data to be re-analyzed as new theoretical viewpoints emerge or as novel statistical techniques are developed. Adoption of these practices will lead to increased credibility of meta-analytic conclusions, and facilitate cumulative scientific knowledge.},
   author = {Daniël Lakens and Joe Hilgard and Janneke Staaks},
   doi = {10.1186/S40359-016-0126-3/TABLES/1},
   issn = {20507283},
   issue = {1},
   journal = {BMC Psychology},
   keywords = {Meta-analysis,Open science,Reporting guidelines,Reproducibility},
   month = {5},
   pages = {1-10},
   pmid = {27241618},
   publisher = {BioMed Central Ltd.},
   title = {On the reproducibility of meta-analyses: Six practical recommendations},
   volume = {4},
   url = {https://bmcpsychology.biomedcentral.com/articles/10.1186/s40359-016-0126-3},
   year = {2016},
}
@web_page{,
   title = {Common statistical tests are linear models (or: how to teach stats)},
   url = {https://lindeloev.github.io/tests-as-linear/},
}
@article{Lorenzetti2013,
   abstract = {Background: Reference management software programs enable researchers to more easily organize and manage large volumes of references typically identified during the production of systematic reviews. The purpose of this study was to determine the extent to which authors are using reference management software to produce systematic reviews; identify which programs are used most frequently and rate their ease of use; and assess the degree to which software usage is documented in published studies. Methods. We reviewed the full text of systematic reviews published in core clinical journals indexed in ACP Journal Club from 2008 to November 2011 to determine the extent to which reference management software usage is reported in published reviews. We surveyed corresponding authors to verify and supplement information in published reports, and gather frequency and ease-of-use data on individual reference management programs. Results: Of the 78 researchers who responded to our survey, 79.5% reported that they had used a reference management software package to prepare their review. Of these, 4.8% reported this usage in their published studies. EndNote, Reference Manager, and RefWorks were the programs of choice for more than 98% of authors who used this software. Comments with respect to ease-of-use issues focused on the integration of this software with other programs and computer interfaces, and the sharing of reference databases among researchers. Conclusions: Despite underreporting of use, reference management software is frequently adopted by authors of systematic reviews. The transparency, reproducibility and quality of systematic reviews may be enhanced through increased reporting of reference management software usage. © 2013 Lorenzetti and Ghali; licensee BioMed Central Ltd.},
   author = {Diane L. Lorenzetti and William A. Ghali},
   doi = {10.1186/1471-2288-13-141/TABLES/2},
   issn = {14712288},
   issue = {1},
   journal = {BMC Medical Research Methodology},
   keywords = {Bibliographic,Data collection,Databases,Meta-analysis,Reference management software,Software,Systematic reviews},
   month = {11},
   pages = {1-5},
   pmid = {24237877},
   publisher = {BioMed Central},
   title = {Reference management software for systematic reviews and meta-analyses: An exploration of usage and usability},
   volume = {13},
   url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-13-141},
   year = {2013},
}
@article{Ma2020,
   abstract = {Methodological quality (risk of bias) assessment is an important step before study initiation usage. Therefore, accurately judging study type is the first priority, and the choosing proper tool is also important. In this review, we introduced methodological quality assessment tools for randomized controlled trial (including individual and cluster), animal study, non-randomized interventional studies (including follow-up study, controlled before-and-after study, before-after/ pre-post study, uncontrolled longitudinal study, interrupted time series study), cohort study, case-control study, cross-sectional study (including analytical and descriptive), observational case series and case reports, comparative effectiveness research, diagnostic study, health economic evaluation, prediction study (including predictor finding study, prediction model impact study, prognostic prediction model study), qualitative study, outcome measurement instruments (including patient - reported outcome measure development, content validity, structural validity, internal consistency, cross-cultural validity/ measurement invariance, reliability, measurement error, criterion validity, hypotheses testing for construct validity, and responsiveness), systematic review and meta-analysis, and clinical practice guideline. The readers of our review can distinguish the types of medical studies and choose appropriate tools. In one word, comprehensively mastering relevant knowledge and implementing more practices are basic requirements for correctly assessing the methodological quality.},
   author = {Lin Lu Ma and Yun Yun Wang and Zhi Hua Yang and Di Huang and Hong Weng and Xian Tao Zeng},
   doi = {10.1186/S40779-020-00238-8/TABLES/1},
   issn = {20549369},
   issue = {1},
   journal = {Military Medical Research},
   keywords = {Appraisal tool,Critical appraisal,Interventional study,Methodological quality,Methodology checklist,Observational study,Outcome measurement instrument,Qualitative study,Quality assessment,Risk of bias},
   month = {2},
   pages = {1-11},
   pmid = {32111253},
   publisher = {BioMed Central Ltd.},
   title = {Methodological quality (risk of bias) assessment tools for primary and secondary medical studies: What are they and which is better?},
   volume = {7},
   url = {https://mmrjournal.biomedcentral.com/articles/10.1186/s40779-020-00238-8},
   year = {2020},
}
@article{Moher2016,
   abstract = {Systematic reviews should build on a protocol that describes the rationale, hypothesis, and planned methods of the review; few reviews report whether a protocol exists. Detailed, well-described protocols can facilitate the understanding and appraisal of the review methods, as well as the detection of modifications to methods and selective reporting in completed reviews. We describe the development of a reporting guideline, the Preferred Reporting Items for Systematic reviews and Meta-Analyses for Protocols 2015 (PRISMA-P 2015). PRISMA-P consists of a 17-item checklist intended to facilitate the preparation and reporting of a robust protocol for the systematic review. Funders and those commissioning reviews might consider mandating the use of the checklist to facilitate the submission of relevant protocol information in funding applications. Similarly, peer reviewers and editors can use the guidance to gauge the completeness and transparency of a systematic review protocol submitted for publication in a journal or other medium.},
   author = {David Moher and Larissa Shamseer and Mike Clarke and Davina Ghersi and Alessandro Liberati and Mark Petticrew and Paul Shekelle and Lesley A. Stewart and Mireia Estarli and Eliud S.Aguilar Barrera and Rodrigo Martínez-Rodríguez and Eduard Baladia and Samuel Duran Agüero and Saby Camacho and Kristian Buhring and Aitor Herrero-López and Diana Maria Gil-González and Douglas G. Altman and Alison Booth and An Wen Chan and Stephanie Chang and Tammy Clifford and Kay Dickersin and Matthias Egger and Peter C. Gøtzsche and Jeremy M. Grimshaw and Trish Groves and Mark Helfand and Julian Higgins and Toby Lasserson and Joseph Lau and Kathleen Lohr and Jessie McGowan and Cynthia Mulrow and Melissa Norton and Matthew Page and Margaret Sampson and Holger Schünemann and Iveta Simera and William Summerskill and Jennifer Tetzlaff and Thomas A. Trikalinos and David Tovey and Lucy Turner and Evelyn Whitlock},
   doi = {10.1186/2046-4053-4-1/TABLES/4},
   issn = {21731292},
   issue = {2},
   journal = {Revista Espanola de Nutricion Humana y Dietetica},
   keywords = {Access to information,Checklist,Evidence-based medicine,Meta-Analysis,Review},
   month = {1},
   pages = {148-160},
   pmid = {25554246},
   publisher = {Asociacion Espanola de Dietistas-Nutricionistas},
   title = {Preferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015 statement},
   volume = {20},
   url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/2046-4053-4-1},
   year = {2016},
}
@article{Lindsay2018,
   author = {Brian A. Nosek and D. Stephen Lindsay},
   journal = {APS Observer},
   keywords = {Preregistration,Statistical Analysis},
   month = {2},
   title = {Preregistration Becoming the Norm in Psychological Science},
   volume = {31},
   url = {https://www.psychologicalscience.org/observer/preregistration-becoming-the-norm-in-psychological-science},
   year = {2018},
}
@article{Page2021,
   abstract = {The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) statement, published in 2009, was designed to help systematic reviewers transparently report why the review was done, what the authors did, and what they found. Over the past decade, advances in systematic review methodology and terminology have necessitated an update to the guideline. The PRISMA 2020 statement replaces the 2009 statement and includes new reporting guidance that reflects advances in methods to identify, select, appraise, and synthesise studies. The structure and presentation of the items have been modified to facilitate implementation. In this article, we present the PRISMA 2020 27-item checklist, an expanded checklist that details reporting recommendations for each item, the PRISMA 2020 abstract checklist, and the revised flow diagrams for original and updated reviews.

Systematic reviews serve many critical roles. They can provide syntheses of the state of knowledge in a field, from which future research priorities can be identified; they can address questions that otherwise could not be answered by individual studies; they can identify problems in primary research that should be rectified in future studies; and they can generate or evaluate theories about how or why phenomena occur. Systematic reviews therefore generate various types of knowledge for different users of reviews (such as patients, healthcare providers, researchers, and policy makers).12 To ensure a systematic review is valuable to users, authors should prepare a transparent, complete, and accurate account of why the review was done, what they did (such as how studies were identified and selected) and what they found (such as characteristics of contributing studies and results of meta-analyses). Up-to-date reporting guidance facilitates authors achieving this.3

The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) statement published in 2009 (hereafter referred to as PRISMA 2009)45678910 …},
   author = {Matthew J. Page and Joanne E. McKenzie and Patrick M. Bossuyt and Isabelle Boutron and Tammy C. Hoffmann and Cynthia D. Mulrow and Larissa Shamseer and Jennifer M. Tetzlaff and Elie A. Akl and Sue E. Brennan and Roger Chou and Julie Glanville and Jeremy M. Grimshaw and Asbjørn Hróbjartsson and Manoj M. Lalu and Tianjing Li and Elizabeth W. Loder and Evan Mayo-Wilson and Steve McDonald and Luke A. McGuinness and Lesley A. Stewart and James Thomas and Andrea C. Tricco and Vivian A. Welch and Penny Whiting and David Moher},
   doi = {10.1136/BMJ.N71},
   issn = {1756-1833},
   journal = {BMJ},
   month = {3},
   pmid = {33782057},
   publisher = {British Medical Journal Publishing Group},
   title = {The PRISMA 2020 statement: an updated guideline for reporting systematic reviews},
   volume = {372},
   url = {https://www.bmj.com/content/372/bmj.n71 https://www.bmj.com/content/372/bmj.n71.abstract},
   year = {2021},
}
@article{Quintana2015,
   abstract = {Meta-analysis synthesizes a body of research investigating a common research question. Outcomes from meta-analyses provide a more objective and transparent summary of a research area than traditional narrative reviews. Moreover, they are often used to support research grant applications, guide clinical practice, and direct health policy. The aim of this article is to provide a practical and non-technical guide for psychological scientists that outlines the steps involved in planning and performing a meta-analysis of correlational datasets. I provide a supplementary R script to demonstrate each analytical step described in the paper, which is readily adaptable for researchers to use for their analyses. While the worked example is the analysis of a correlational dataset, the general meta-analytic process described in this paper is applicable for all types of effect sizes. I also emphasize the importance of meta-analysis protocols and pre-registration to improve transparency and help avoid unintended duplication. An improved understanding this tool will not only help scientists to conduct their own meta-analyses but also improve their evaluation of published meta-analyses.},
   author = {Daniel S. Quintana},
   doi = {10.3389/FPSYG.2015.01549/BIBTEX},
   issn = {16641078},
   issue = {OCT},
   journal = {Frontiers in Psychology},
   keywords = {Meta-analysis,Methods,Pre-registration,Primer,Publication bias,Statistics},
   month = {10},
   pages = {1549},
   publisher = {Frontiers Media S.A.},
   title = {From pre-registration to publication: A non-technical primer for conducting a meta-analysis to synthesize correlational data},
   volume = {6},
   year = {2015},
}
@article{Renkewitz2019,
   abstract = {Abstract. Publication biases and questionable research practices are assumed to be two of the main causes of low replication rates. Both of these problems lead to severely inflated effect size esti...},
   author = {Frank Renkewitz and Melanie Keiner},
   doi = {10.1027/2151-2604/A000386},
   issn = {21512604},
   issue = {4},
   journal = {https://doi.org/10.1027/2151-2604/a000386},
   keywords = {bias detection,heterogeneity,meta-analysis,optional stopping,publication bias},
   month = {12},
   pages = {261-279},
   publisher = { Hogrefe Publishing },
   title = {How to Detect Publication Bias in Psychological Research},
   volume = {227},
   url = {https://econtent.hogrefe.com/doi/10.1027/2151-2604/a000386},
   year = {2019},
}
@article{Stanley2017,
   abstract = {A novel meta-regression method, precision-effect test and precision-effect estimate with standard errors (PET-PEESE), predicts and explains recent high-profile failures to replicate in psychology. The central purpose of this article is to identify the limitations of PET-PEESE for application to social/personality psychology. Using typical conditions found in social/personality research, our simulations identify three areas of concern. PET-PEESE performs poorly in research areas where there are only a few studies, all studies use small samples, and where there is very high heterogeneity of results from study to study. Nonetheless, the statistical properties of conventional meta-analysis approaches are much worse than PET-PEESE under these same conditions. Our simulations suggest alterations to conventional research practice and ways to moderate PET-PEESE weaknesses.},
   author = {T. D. Stanley},
   doi = {10.1177/1948550617693062/ASSET/IMAGES/LARGE/10.1177_1948550617693062-FIG2.JPEG},
   issn = {19485514},
   issue = {5},
   journal = {Social Psychological and Personality Science},
   keywords = {PET-PEESE,meta-analysis,publication bias,random effects,weighted least squares},
   month = {7},
   pages = {581-591},
   publisher = {SAGE Publications Inc.},
   title = {Limitations of PET-PEESE and Other Meta-Analysis Methods},
   volume = {8},
   url = {https://journals.sagepub.com/doi/10.1177/1948550617693062},
   year = {2017},
}
@article{,
   abstract = {Our job as scientists is to discover truths about the world. We generate hypotheses, collect data, and examine whether or not the data are consistent with those hypotheses. Although we aspire to always be accurate, errors are inevitable. Perhaps the most costly error is a false positive, the incorrect rejection of a null hypothesis. First, once they appear in the literature, false positives are particularly persistent. Because null results have many possible causes, failures to replicate previous findings are never conclusive. Furthermore, because it is uncommon for prestigious journals to publish null findings or exact replications, researchers have little incentive to even attempt them. Second, false positives waste resources: They inspire investment in fruitless research programs and can lead to ineffective policy changes. Finally, a field known for publishing false positives risks losing its credibility. In this article, we show that despite the nominal endorsement of a maximum false-positive rate of 5% (i.e., p ≤ .05), current standards for disclosing details of data collection and analyses make false positives vastly more likely. In fact, it is unacceptably easy to publish "statistically significant" evidence consistent with any hypothesis. The culprit is a construct we refer to as researcher degrees of freedom. In the course of collecting and analyzing data, researchers have many decisions to make: Should more data be collected? Should some observations be excluded? Which conditions should be combined and which ones compared? Which control variables should be considered? Should specific measures be combined or transformed or both? It is rare, and sometimes impractical, for researchers to make all these decisions beforehand. Rather, it is common (and accepted practice) for researchers to explore various analytic alternatives, to search for a combination that yields "sta-tistical significance," and to then report only what "worked." The problem, of course, is that the likelihood of at least one (of many) analyses producing a falsely positive finding at the 5% level is necessarily greater than 5%. This exploratory behavior is not the by-product of malicious intent, but rather the result of two factors: (a) ambiguity in how best to make these decisions and (b) the researcher's desire to find a statistically significant result. A large literature documents that people are self-serving in their interpretation Abstract In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
   author = {Joseph P Simmons and Leif D Nelson and Uri Simonsohn},
   doi = {10.1177/0956797611417632},
   issue = {11},
   journal = {Psychological Science},
   keywords = {disclosure,methodology,motivated reasoning,publication},
   pages = {1359-1366},
   title = {False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant},
   volume = {22},
   url = {http://pss.sagepub.com},
}
@article{Stern1997,
   abstract = {Objectives: To determine the extent to which publication is influenced by study outcome.

Design: A cohort of studies submitted to a hospital ethics committee over 10 years were examined retrospectively by reviewing the protocols and by questionnaire. The primary method of analysis was Cox's proportional hazards model.

Setting: University hospital, Sydney, Australia.

Studies: 748 eligible studies submitted to Royal Prince Alfred Hospital Ethics Committee between 1979 and 1988.

Main outcome measures: Time to publication.

Results: Response to the questionnaire was received for 520 (70%) of the eligible studies. Of the 218 studies analysed with tests of significance, those with positive results (P<0.05) were much more likely to be published than those with negative results (P0.10) (hazard ratio 2.32 (95% confidence interval 1.47 to 3.66), P=0.0003), with a significantly shorter time to publication (median 4.8 v 8.0 years). This finding was even stronger for the group of 130 clinical trials (hazard ratio 3.13 (1.76 to 5.58), P=0.0001), with median times to publication of 4.7 and 8.0 years respectively. These results were not materially changed after adjusting for other significant predictors of publication. Studies with indefinite conclusions (0.05 P<0.10) tended to have an even lower publication rate and longer time to publication than studies with negative results (hazard ratio 0.39 (0.13 to 1.12), P=0.08). For the 103 studies in which outcome was rated qualitatively, there was no clear cut evidence of publication bias, although the number of studies in this group was not large.

Conclusions: This study confirms the evidence of publication bias found in other studies and identifies delay in publication as an additional important factor. The study results support the need for prospective registration of trials to avoid publication bias and also support restricting the selection of trials to those started before a common date in undertaking systematic reviews.

#### Key messages},
   author = {Jerome M. Stern and R. John Simes},
   doi = {10.1136/BMJ.315.7109.640},
   issn = {0959-8138},
   issue = {7109},
   journal = {BMJ},
   month = {9},
   pages = {640-645},
   pmid = {9310565},
   publisher = {British Medical Journal Publishing Group},
   title = {Publication bias: evidence of delayed publication in a cohort study of clinical research projects},
   volume = {315},
   url = {https://www.bmj.com/content/315/7109/640 https://www.bmj.com/content/315/7109/640.abstract},
   year = {1997},
}
@article{Veroniki2016,
   abstract = {Meta-analyses are typically used to estimate the overall/mean of an outcome of interest. However, inference about between-study variability, which is typically modelled using a between-study variance parameter, is usually an additional aim. The DerSimonian and Laird method, currently widely used by default to estimate the between-study variance, has been long challenged. Our aim is to identify known methods for estimation of the between-study variance and its corresponding uncertainty, and to summarise the simulation and empirical evidence that compares them. We identified 16 estimators for the between-study variance, seven methods to calculate confidence intervals, and several comparative studies. Simulation studies suggest that for both dichotomous and continuous data the estimator proposed by Paule and Mandel and for continuous data the restricted maximum likelihood estimator are better alternatives to estimate the between-study variance. Based on the scenarios and results presented in the published studies, we recommend the Q-profile method and the alternative approach based on a 'generalised Cochran between-study variance statistic' to compute corresponding confidence intervals around the resulting estimates. Our recommendations are based on a qualitative evaluation of the existing literature and expert consensus. Evidence-based recommendations require an extensive simulation study where all methods would be compared under the same scenarios.},
   author = {Areti Angeliki Veroniki and Dan Jackson and Wolfgang Viechtbauer and Ralf Bender and Jack Bowden and Guido Knapp and Oliver Kuss and Julian Pt Higgins and Dean Langan and Georgia Salanti},
   doi = {10.1002/JRSM.1164},
   issn = {1759-2887},
   issue = {1},
   journal = {Research Synthesis Methods},
   keywords = {bias,confidence interval,coverage probability,heterogeneity,mean squared error},
   month = {3},
   pages = {55-79},
   pmid = {26332144},
   publisher = {John Wiley & Sons, Ltd},
   title = {Methods to estimate the between-study variance and its uncertainty in meta-analysis},
   volume = {7},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/jrsm.1164 https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1164 https://onlinelibrary.wiley.com/doi/10.1002/jrsm.1164},
   year = {2016},
}
@article{Viechtbauer2010,
   abstract = {The metafor package provides functions for conducting meta-analyses in R. The package includes functions for fitting the meta-analytic fixed- and random-effects models and allows for the inclusion of moderators variables (study-level covariates) in these models. Meta-regression analyses with continuous and categorical moderators can be conducted in this way.  Functions for the Mantel-Haenszel and Peto's one-step method for meta-analyses of 2 x 2 table data are also available. Finally, the package provides various plot functions (for example, for forest, funnel, and radial plots) and functions for assessing the model fit, for obtaining case diagnostics, and for tests of publication bias.},
   author = {Wolfgang Viechtbauer},
   doi = {10.18637/JSS.V036.I03},
   issn = {1548-7660},
   issue = {3},
   journal = {Journal of Statistical Software},
   month = {8},
   pages = {1-48},
   title = {Conducting Meta-Analyses in R with the metafor Package},
   volume = {36},
   url = {https://www.jstatsoft.org/index.php/jss/article/view/v036i03},
   year = {2010},
}
@article{Viechtbauer2010,
   abstract = {The presence of outliers and influential cases may affect the validity and robustness of the conclusions from a meta-analysis. While researchers generally agree that it is necessary to examine outlier and influential case diagnostics when conducting a meta-analysis, limited studies have addressed how to obtain such diagnostic measures in the context of a meta-analysis. The present paper extends standard diagnostic procedures developed for linear regression analyses to the meta-analytic fixed- and random/mixed-effects models. Three examples are used to illustrate the usefulness of these procedures in various research settings. Issues related to these diagnostic procedures in meta-analysis are also discussed. Copyright © 2010 John Wiley & Sons, Ltd.},
   author = {Wolfgang Viechtbauer and Mike W.-L. Cheung},
   doi = {10.1002/JRSM.11},
   issn = {1759-2887},
   issue = {2},
   journal = {Research Synthesis Methods},
   keywords = {analysis,effects model,influence diagnostics,meta,mixed,outliers},
   month = {4},
   pages = {112-125},
   pmid = {26061377},
   publisher = {John Wiley & Sons, Ltd},
   title = {Outlier and influence diagnostics for meta-analysis},
   volume = {1},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/jrsm.11 https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.11 https://onlinelibrary.wiley.com/doi/10.1002/jrsm.11},
   year = {2010},
}
@article{Watt2017,
   author = {Caroline A. Watt and James E. Kennedy},
   doi = {10.3389/FPSYG.2016.02030/BIBTEX},
   issn = {16641078},
   issue = {JAN},
   journal = {Frontiers in Psychology},
   keywords = {Confirmatory research,Meta-analysis,Prospective meta-analysis,Replication,Researcher bias,Study registration},
   month = {1},
   pages = {2030},
   publisher = {Frontiers Media S.A.},
   title = {Options for prospective meta-analysis and introduction of registration-based prospective meta-analysis},
   volume = {7},
   year = {2017},
}
@article{FIELD2013,
   author = {ANDY. MILES FIELD},
   isbn = {9781446289136},
   publisher = {SAGE PUBLICATIONS},
   title = {DISCOVERING STATISTICS USING R.},
   year = {2013},
}
@article{,
   title = {What is……Multilevel Modelling Vs Fixed Effects Will Cook Social Statistics},
}
